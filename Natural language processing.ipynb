{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'data': File exists\n",
      "Archive:  data/ptb.zip\n",
      "  inflating: data/ptb/reader.py      \n",
      "  inflating: data/__MACOSX/ptb/._reader.py  \n",
      "  inflating: data/__MACOSX/._ptb     \n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "!unzip -o data/ptb.zip -d data\n",
    "!cp data/ptb/reader.py .\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-13 06:27:15--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: 'simple-examples.tgz.2'\n",
      "\n",
      "100%[======================================>] 34,869,662  3.92MB/s   in 9.9s   \n",
      "\n",
      "2020-01-13 06:27:25 (3.34 MB/s) - 'simple-examples.tgz.2' saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 60\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size = 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0113 06:28:11.734205 140735717985136 module_wrapper.py:139] From /home/user606/anaconda3/reader.py:30: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,\n",
       "           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {_input_data:x, _targets:y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, ..., 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361, ...,  241,   13, 2420],\n",
       "       [2654,    6,  334, ...,  514,    8,  605],\n",
       "       ...,\n",
       "       [7831,   36, 1678, ...,    4, 4558,  157],\n",
       "       [  59, 2070, 2433, ...,  400,    1, 1173],\n",
       "       [2097,    3,    2, ..., 2043,   23,    1]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0113 06:28:40.861029 140735717985136 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0113 06:28:42.054468 140735717985136 deprecation.py:323] From <ipython-input-15-ddf565d0e753>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0113 06:28:42.058763 140735717985136 deprecation.py:323] From <ipython-input-15-ddf565d0e753>:3: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(60, 256) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(60, 256) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(60, 128) dtype=float32>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_initial_state, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])  #[10000x200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00402603,  0.01025365, -0.01624187, ..., -0.01110291,\n",
       "         0.02134619,  0.01585925],\n",
       "       [ 0.00911555,  0.00953951,  0.02367998, ..., -0.00192591,\n",
       "        -0.00979596,  0.01117923],\n",
       "       [ 0.00294629,  0.01141051,  0.00707472, ..., -0.01871925,\n",
       "         0.01482446, -0.02187183],\n",
       "       ...,\n",
       "       [ 0.00316672, -0.02182113, -0.00607728, ..., -0.02028207,\n",
       "         0.01299785, -0.00913576],\n",
       "       [ 0.0001412 ,  0.0190626 , -0.01464759, ..., -0.01093327,\n",
       "         0.0240654 ,  0.00129859],\n",
       "       [-0.00512628, -0.01350387, -0.00936851, ...,  0.00201112,\n",
       "         0.00405062, -0.00492592]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(60, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  #shape=(30, 20, 200) \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01046724, -0.01809578, -0.02209293, ...,  0.01033784,\n",
       "        -0.00318746, -0.02128394],\n",
       "       [-0.00139767, -0.01084377, -0.01242592, ...,  0.00339912,\n",
       "        -0.01673359, -0.00716353],\n",
       "       [-0.01232735,  0.01996412, -0.00054254, ..., -0.01184142,\n",
       "         0.00246683,  0.01629991],\n",
       "       ...,\n",
       "       [-0.01100942,  0.0135478 ,  0.01731201, ...,  0.01494781,\n",
       "         0.01253283,  0.01985766],\n",
       "       [-0.01278169, -0.02207025,  0.00168847, ..., -0.00195309,\n",
       "         0.01794595,  0.01412597],\n",
       "       [ 0.02270446,  0.01833266,  0.01098362, ..., -0.01163286,\n",
       "        -0.02228716,  0.00781219]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0113 06:28:53.420618 140735717985136 deprecation.py:323] From <ipython-input-22-b4cc29283025>:1: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0113 06:28:53.470549 140735717985136 deprecation.py:323] From /home/user606/anaconda3/envs/myclone/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "W0113 06:28:53.478603 140735717985136 deprecation.py:506] From /home/user606/anaconda3/envs/myclone/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(60, 20, 128) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.9881758e-04,  2.5795249e-04,  2.1879964e-04, ...,\n",
       "        -2.8022207e-04,  1.6563277e-04, -1.4495780e-04],\n",
       "       [-3.7393282e-04,  1.6346312e-04,  2.2255612e-04, ...,\n",
       "        -7.7221321e-04, -6.2076826e-05,  2.9716865e-04],\n",
       "       [-5.4180600e-05,  2.1582755e-04,  8.3255480e-05, ...,\n",
       "        -5.5567030e-04, -2.3769737e-04,  3.8791893e-04],\n",
       "       ...,\n",
       "       [-9.8660565e-04, -2.2768143e-04, -7.8514051e-05, ...,\n",
       "         6.5292275e-05,  1.1311491e-04, -6.5711953e-05],\n",
       "       [-1.1282342e-03, -8.6506753e-04, -3.9863054e-04, ...,\n",
       "         1.8734575e-04, -2.8657872e-04,  8.9468456e-05],\n",
       "       [-1.5629095e-03, -1.2950113e-03, -4.8526769e-04, ...,\n",
       "        -1.3653030e-04, -1.6643017e-04, -2.8418376e-07]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(outputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(1200, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[200x1000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output:  (1200, 10000)\n",
      "The probability of observing words in t=0 to t=20 [[9.94127840e-05 9.89236505e-05 9.93136564e-05 ... 1.00070582e-04\n",
      "  1.00895391e-04 9.90105254e-05]\n",
      " [9.94111324e-05 9.89290565e-05 9.93021240e-05 ... 1.00070749e-04\n",
      "  1.00896272e-04 9.90116969e-05]\n",
      " [9.94111542e-05 9.89394248e-05 9.92876376e-05 ... 1.00076395e-04\n",
      "  1.00893754e-04 9.90048065e-05]\n",
      " ...\n",
      " [9.94204383e-05 9.89257314e-05 9.93292706e-05 ... 1.00085388e-04\n",
      "  1.00876976e-04 9.90201734e-05]\n",
      " [9.94091170e-05 9.89286127e-05 9.93364156e-05 ... 1.00080899e-04\n",
      "  1.00879013e-04 9.90220506e-05]\n",
      " [9.94035363e-05 9.89302353e-05 9.93441208e-05 ... 1.00088801e-04\n",
      "  1.00877238e-04 9.90238914e-05]]\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "output_words_prob = session.run(prob, feed_dict)\n",
    "print(\"shape of the output: \", output_words_prob.shape)\n",
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5548, 5548, 3198, 3198, 5235, 5235, 5235, 1273, 2077, 1310, 1310,\n",
       "       1310, 9556, 9556, 2871, 1460, 1460, 1460, 1460, 1460])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0:20], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = session.run(_targets, feed_dict) \n",
    "targ[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.207124, 9.196023, 9.217647, 9.202679, 9.201848, 9.221031,\n",
       "       9.218178, 9.19978 , 9.218396, 9.201445], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(loss, feed_dict)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.24074"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(cost, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "# Create the gradient descent optimizer with our learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_vocab:0' shape=(10000, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(456, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(128, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',\n",
       " 'softmax_w:0',\n",
       " 'softmax_b:0']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = tf.placeholder(tf.float32)\n",
    "var_y = tf.placeholder(tf.float32) \n",
    "func_test = 2.0 * var_x * var_x + 3.0 * var_x * var_y\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(func_test, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_x])\n",
    "session.run(var_grad, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_y])\n",
    "session.run(var_grad, {var_x:1.0, var_y:2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x7ffec0100780>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/MatMul_grad/MatMul_1:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/add_grad/Reshape:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gradients(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "#sess.run(grad_t_list,feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0113 06:29:20.054924 140735717985136 deprecation.py:323] From /home/user606/anaconda3/envs/myclone/lib/python3.6/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x7ffec00b3208>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndexedSlicesValue(values=array([[ 2.6976943e-06, -3.9866841e-06,  8.7007611e-06, ...,\n",
       "         -2.3923255e-06, -4.6447144e-06, -8.1872940e-06],\n",
       "        [-6.6009225e-06,  3.7917508e-07,  1.6512024e-05, ...,\n",
       "          7.7939412e-06, -4.5274082e-06, -6.9122552e-06],\n",
       "        [-7.4850145e-06,  3.0210665e-06,  2.0848805e-05, ...,\n",
       "          1.2422064e-05, -3.5228422e-06, -6.9200851e-06],\n",
       "        ...,\n",
       "        [-7.9121364e-06,  9.7251041e-06,  2.9170023e-06, ...,\n",
       "         -5.0646927e-06,  1.4034755e-06,  2.7348797e-06],\n",
       "        [-5.2227583e-06, -1.0192498e-06, -1.0579127e-05, ...,\n",
       "          7.0027471e-07,  3.3368401e-06,  6.2737054e-06],\n",
       "        [-3.3293579e-06, -3.7760815e-06, -3.2178364e-06, ...,\n",
       "          1.4040510e-06, -1.7585530e-06,  7.3279339e-06]], dtype=float32), indices=array([9970, 9971, 9972, ..., 2043,   23,    1], dtype=int32), dense_shape=array([10000,   200], dtype=int32)),\n",
       " array([[-6.5843686e-08, -6.2990651e-08, -1.4325832e-09, ...,\n",
       "          5.2611515e-09, -5.2387441e-09, -2.1254445e-08],\n",
       "        [-1.7796314e-07, -4.4518725e-08,  3.7272425e-08, ...,\n",
       "          9.8408988e-09,  3.2391146e-08, -7.5553190e-09],\n",
       "        [-1.5048716e-08,  5.7845384e-08, -4.5912669e-09, ...,\n",
       "          2.1156950e-08, -1.4259394e-09, -3.4654764e-08],\n",
       "        ...,\n",
       "        [ 3.5322096e-09, -3.0896496e-09, -6.1872854e-09, ...,\n",
       "         -4.3947024e-09, -1.5915899e-10, -1.8839379e-09],\n",
       "        [-6.7980743e-10, -2.7703293e-09, -9.6258512e-10, ...,\n",
       "          3.1240459e-09,  1.1320410e-08, -3.2380030e-09],\n",
       "        [-4.9726050e-09,  1.7326865e-09, -6.3753545e-09, ...,\n",
       "         -1.2552570e-09, -2.1315496e-09, -1.7672501e-09]], dtype=float32),\n",
       " array([-1.2624991e-05, -1.6499328e-06, -4.6600081e-07, ...,\n",
       "        -8.6593707e-07,  1.4600099e-07,  9.5773589e-07], dtype=float32),\n",
       " array([[-7.8281104e-10,  9.4060812e-09,  7.4024600e-09, ...,\n",
       "          4.4230299e-09,  2.6640667e-09,  2.7693130e-09],\n",
       "        [-7.6065171e-10, -4.0624770e-09, -1.8279848e-09, ...,\n",
       "          1.2802235e-08,  3.0561447e-09, -2.9355878e-09],\n",
       "        [-8.4936538e-09,  1.6452569e-08,  9.3131280e-09, ...,\n",
       "         -3.6781225e-09,  2.0075162e-09,  8.3948679e-09],\n",
       "        ...,\n",
       "        [-1.6345567e-10,  5.6088106e-11, -3.6261740e-09, ...,\n",
       "         -2.0722614e-10,  2.4691466e-10,  3.6792100e-09],\n",
       "        [ 1.2849350e-09,  5.2361010e-10, -1.6130906e-09, ...,\n",
       "         -4.6252135e-10,  2.2363955e-09, -4.1634278e-09],\n",
       "        [ 1.9854912e-10,  7.2827028e-10, -6.1221594e-10, ...,\n",
       "          1.5402561e-09, -4.9311405e-10,  7.9007201e-09]], dtype=float32),\n",
       " array([-1.96335031e-06, -3.60215790e-06, -1.24898565e-06,  1.36058588e-06,\n",
       "         3.24096368e-06, -1.40609302e-08,  5.78653589e-06, -2.97733686e-06,\n",
       "         1.83374118e-06,  3.33021057e-06,  7.13891268e-06, -4.68382268e-06,\n",
       "         8.52197729e-07, -1.57594582e-06, -2.49996515e-06,  1.25014049e-06,\n",
       "        -5.42871760e-07, -3.37551933e-06, -4.02288151e-06, -7.31702130e-07,\n",
       "         6.34124319e-07, -2.03744685e-06, -3.29188924e-06,  3.06770903e-06,\n",
       "        -3.45735680e-06,  6.70946156e-06, -1.46435787e-06,  2.41599673e-06,\n",
       "        -4.52538052e-06, -1.80497716e-06, -4.40778058e-06,  3.58049715e-06,\n",
       "        -2.79828646e-06, -6.18303375e-06,  4.10375651e-06,  2.09530231e-06,\n",
       "        -2.08767688e-06, -1.86205625e-07, -1.52207303e-06, -1.89873606e-06,\n",
       "        -2.05076140e-06,  3.67001803e-07,  4.24664336e-07, -1.40870597e-07,\n",
       "        -1.18034791e-06, -3.87776345e-06,  4.24892789e-07, -1.49512209e-06,\n",
       "        -3.72060185e-06, -2.45344324e-07, -1.34760273e-06,  3.88342551e-06,\n",
       "         3.90835885e-06,  1.52935445e-06, -7.64507058e-06, -2.85461851e-06,\n",
       "         2.35044718e-06,  1.92832249e-06, -3.60834883e-06, -2.06105005e-06,\n",
       "        -6.31903140e-06,  8.35265098e-07,  2.65098288e-06,  1.94418305e-07,\n",
       "        -3.96965561e-06, -1.02641332e-06,  5.32193553e-06,  8.09149356e-07,\n",
       "        -2.53382694e-07,  1.04751052e-09, -2.98017312e-06, -1.42642887e-06,\n",
       "         4.43433635e-07, -3.08048334e-06, -8.85270219e-06,  4.61382172e-07,\n",
       "         6.06914091e-06, -2.03296213e-06, -3.13712212e-06,  5.37265441e-06,\n",
       "        -3.73742819e-06, -7.97378743e-06, -4.59744388e-06,  7.53229813e-07,\n",
       "         3.37059100e-06, -5.96056111e-07, -1.98744544e-07,  1.31758434e-05,\n",
       "         1.25871202e-06, -5.24873428e-07, -2.55491636e-06, -5.13252303e-07,\n",
       "        -4.32222350e-06,  1.48764013e-06,  1.49145853e-06, -2.43980435e-06,\n",
       "         2.64451978e-06, -4.41497662e-07,  3.63597610e-06, -1.63563783e-07,\n",
       "         3.00914462e-06, -1.75228001e-06,  1.78910045e-06, -2.78316543e-06,\n",
       "         2.69834027e-06, -1.72842476e-06,  3.55534439e-06, -1.41025373e-06,\n",
       "        -3.61027014e-06,  1.88459580e-06, -2.05620540e-06,  1.67631913e-06,\n",
       "         1.11627935e-06, -1.70674934e-06,  3.40643646e-06,  1.39261101e-06,\n",
       "         5.31184014e-06,  2.54873339e-06,  1.60924117e-06,  1.79242693e-06,\n",
       "         3.26272698e-07, -3.38856353e-06,  1.49524533e-06,  3.53838118e-06,\n",
       "         6.16381658e-06,  3.64447942e-06, -2.00022941e-06,  1.49689095e-06,\n",
       "         8.12493544e-03, -8.98918696e-03,  2.03440730e-02, -2.28449311e-02,\n",
       "         4.12967848e-03, -9.67532396e-03, -3.25495116e-02, -2.92515964e-04,\n",
       "         1.84250548e-02,  3.34139578e-02, -2.10271105e-02, -5.63439541e-03,\n",
       "        -1.56146921e-02, -8.79708678e-03,  4.47750324e-04, -1.50996400e-03,\n",
       "         2.24144990e-03,  4.22676327e-04,  7.85051659e-03, -5.21012535e-03,\n",
       "         3.01014520e-02,  2.09274162e-02,  1.57634094e-02, -2.17059702e-02,\n",
       "         4.35644435e-03,  3.13297473e-03,  3.46088945e-03,  1.23604657e-02,\n",
       "         2.24626176e-02, -1.08187394e-02, -3.37210968e-02,  4.82919533e-03,\n",
       "        -1.96331106e-02,  1.39961559e-02,  1.09277172e-02,  3.14048229e-04,\n",
       "         2.32588430e-03, -2.60629524e-02,  7.12109497e-03, -1.35950614e-02,\n",
       "        -1.71656273e-02,  1.66470259e-02,  1.89160870e-04,  1.17399404e-02,\n",
       "        -6.35952130e-03,  2.20979960e-03,  1.28308572e-02, -3.36572877e-03,\n",
       "        -2.04946306e-02,  6.00050576e-03,  8.01496860e-03,  3.48068997e-02,\n",
       "        -2.06726864e-02,  1.88219547e-02, -9.04288422e-03, -2.40657199e-02,\n",
       "         5.79381362e-03,  2.39000493e-03,  2.37841588e-02,  1.40407253e-02,\n",
       "         4.44344319e-02,  3.02916691e-02, -4.56147874e-03,  7.69967155e-04,\n",
       "         1.90674013e-03,  4.25327709e-03,  3.30416998e-03, -7.52376998e-03,\n",
       "         1.06183570e-02, -3.08299274e-03,  1.07141659e-02, -1.63542796e-02,\n",
       "         2.64658360e-03, -3.16633470e-03,  1.95675846e-02,  1.13502331e-02,\n",
       "        -1.24837412e-02, -7.83373718e-04,  2.23212391e-02, -2.47533917e-02,\n",
       "         3.42750107e-03, -2.59901807e-02,  6.74043549e-04, -1.53750582e-02,\n",
       "        -6.08197507e-03,  1.23917339e-02, -1.32658668e-02,  2.95414329e-02,\n",
       "        -2.97551509e-03, -3.80765251e-03,  7.56599987e-03,  6.72654854e-03,\n",
       "         1.50294211e-02,  2.37289933e-03, -7.65096769e-03, -1.45172542e-02,\n",
       "        -1.81061942e-02, -8.18138570e-03,  1.19885579e-02,  1.33624915e-02,\n",
       "         7.53424130e-03,  2.94637727e-03,  2.10218737e-03,  1.63416117e-02,\n",
       "         1.59222335e-02, -4.09869384e-03, -5.35894278e-03,  2.77507529e-02,\n",
       "        -3.68477055e-03, -9.51122399e-03,  1.19571742e-02,  6.68859249e-03,\n",
       "         2.29025297e-02, -2.24472843e-02,  4.73395176e-03,  1.14280982e-02,\n",
       "         1.68850832e-02,  1.33669022e-02, -2.02792999e-03,  2.69114599e-03,\n",
       "        -9.50968824e-03, -1.46652828e-03, -1.13022560e-02,  9.76986811e-03,\n",
       "        -7.34342402e-03, -7.27028307e-03, -5.94575377e-03,  3.00571434e-02,\n",
       "        -2.56417479e-06, -4.67531777e-07, -1.02183560e-06, -2.10096232e-06,\n",
       "         1.20036270e-06,  1.24598466e-06,  4.84336715e-06, -4.62657908e-07,\n",
       "        -3.79801520e-07,  1.66510790e-06,  5.47167747e-06, -2.73081832e-06,\n",
       "         9.34812192e-07, -2.74628496e-06, -1.78299956e-06,  1.82186011e-06,\n",
       "        -1.15965895e-06,  1.69978627e-08, -5.09794245e-06,  5.59608281e-07,\n",
       "        -3.98416034e-07, -1.03106140e-06, -1.61001697e-06,  6.39240511e-07,\n",
       "        -3.66329402e-07,  4.16198873e-06,  1.20019752e-06,  1.75609682e-06,\n",
       "        -4.39488304e-06,  1.91180931e-07, -5.04026093e-06,  1.36435870e-06,\n",
       "        -3.55640441e-06, -1.80958835e-07,  2.45459728e-06,  1.47345327e-06,\n",
       "        -1.12919679e-06,  2.01588676e-07, -1.10800431e-06,  2.17959268e-06,\n",
       "         5.67231154e-07,  1.57944942e-06, -1.86849093e-07,  1.43261644e-07,\n",
       "        -1.04901608e-06, -2.43547902e-06,  2.24204882e-07, -2.53502139e-06,\n",
       "        -5.99285045e-07,  1.91306503e-06,  5.55010615e-08,  9.64554829e-07,\n",
       "         2.80286395e-06,  2.24635619e-06, -4.13917223e-06, -2.73185287e-06,\n",
       "         2.05396987e-06,  2.71539147e-06, -6.14211558e-07, -1.19109075e-06,\n",
       "        -5.65301298e-06,  2.71425756e-06,  1.41403302e-06,  1.85272995e-06,\n",
       "        -1.13271722e-06, -2.39014270e-07,  3.69562758e-06, -5.24566872e-07,\n",
       "         1.73313958e-06,  9.58464625e-07, -1.65105826e-06, -6.17957255e-07,\n",
       "        -2.24510245e-07, -2.78470111e-06, -6.72803662e-06,  5.72461602e-07,\n",
       "         4.20187007e-06, -3.65097480e-06, -3.12439033e-06,  4.86152067e-06,\n",
       "        -1.06083087e-06, -4.50666539e-06, -3.64533184e-06, -1.40933514e-06,\n",
       "         1.35877735e-06, -1.30180513e-06,  2.53585335e-06,  8.87194801e-06,\n",
       "        -1.23340999e-06, -8.70842712e-07, -8.52491951e-07,  1.09955977e-06,\n",
       "        -4.50577090e-06, -2.69003408e-06, -2.07752180e-07, -3.95527559e-06,\n",
       "         4.98441250e-06, -5.79894163e-07,  2.32528805e-06,  1.11697727e-06,\n",
       "        -1.19694107e-06, -1.60187528e-06,  2.68916688e-06,  4.53086159e-07,\n",
       "         2.52061454e-06, -1.33749688e-06,  5.41133613e-06, -6.22489949e-07,\n",
       "        -1.29762259e-06, -5.09120639e-07,  8.92211233e-07,  2.53679605e-06,\n",
       "         1.67978862e-06,  1.29904151e-06,  3.83004317e-06,  6.95776464e-07,\n",
       "         2.09955488e-06, -5.60116518e-07,  1.50054927e-06, -9.85662155e-07,\n",
       "         4.55202826e-06, -3.99407691e-06, -9.60513717e-07,  2.09536415e-06,\n",
       "         3.36576227e-06,  1.49068762e-06,  2.01347052e-06,  2.24071073e-06,\n",
       "        -1.96116457e-06, -3.60300487e-06, -1.24505766e-06,  1.36167807e-06,\n",
       "         3.24367761e-06, -1.63361165e-08,  5.78605295e-06, -2.98137888e-06,\n",
       "         1.83331770e-06,  3.32402465e-06,  7.14189900e-06, -4.68262033e-06,\n",
       "         8.55604696e-07, -1.57548550e-06, -2.50058883e-06,  1.25253086e-06,\n",
       "        -5.40932092e-07, -3.37980941e-06, -4.02551041e-06, -7.34562548e-07,\n",
       "         6.32305841e-07, -2.03489071e-06, -3.28953456e-06,  3.07011987e-06,\n",
       "        -3.46154820e-06,  6.71024009e-06, -1.46260197e-06,  2.41770567e-06,\n",
       "        -4.52695258e-06, -1.80467259e-06, -4.39935911e-06,  3.58019838e-06,\n",
       "        -2.79724691e-06, -6.17954629e-06,  4.10648272e-06,  2.09388941e-06,\n",
       "        -2.08600250e-06, -1.90917717e-07, -1.52071323e-06, -1.89592208e-06,\n",
       "        -2.04876551e-06,  3.65667972e-07,  4.20673075e-07, -1.46659744e-07,\n",
       "        -1.18416460e-06, -3.87646878e-06,  4.25893063e-07, -1.49700008e-06,\n",
       "        -3.72121599e-06, -2.46862356e-07, -1.34536708e-06,  3.87956607e-06,\n",
       "         3.91008371e-06,  1.52649204e-06, -7.64133802e-06, -2.85357282e-06,\n",
       "         2.34826666e-06,  1.93449068e-06, -3.60976401e-06, -2.06281243e-06,\n",
       "        -6.32054889e-06,  8.34766070e-07,  2.65370272e-06,  1.93784956e-07,\n",
       "        -3.96786891e-06, -1.02992146e-06,  5.31841260e-06,  8.10565325e-07,\n",
       "        -2.48199456e-07, -1.82799909e-09, -2.97701354e-06, -1.42900876e-06,\n",
       "         4.43950171e-07, -3.08336257e-06, -8.84733436e-06,  4.62074922e-07,\n",
       "         6.06726371e-06, -2.02868250e-06, -3.13641385e-06,  5.37521373e-06,\n",
       "        -3.73860848e-06, -7.97721077e-06, -4.59813464e-06,  7.53429106e-07,\n",
       "         3.37592905e-06, -5.91704406e-07, -1.99189230e-07,  1.31634451e-05,\n",
       "         1.26075872e-06, -5.20457320e-07, -2.55177179e-06, -5.10824236e-07,\n",
       "        -4.32423030e-06,  1.48921754e-06,  1.49338121e-06, -2.43946874e-06,\n",
       "         2.64127243e-06, -4.45014820e-07,  3.63678032e-06, -1.59018725e-07,\n",
       "         3.00692477e-06, -1.75842354e-06,  1.78975756e-06, -2.77946651e-06,\n",
       "         2.70416467e-06, -1.73277795e-06,  3.55448378e-06, -1.41174303e-06,\n",
       "        -3.60904528e-06,  1.88218280e-06, -2.05516244e-06,  1.67359190e-06,\n",
       "         1.11143015e-06, -1.70936676e-06,  3.41106761e-06,  1.39406484e-06,\n",
       "         5.31340356e-06,  2.54475367e-06,  1.60806803e-06,  1.78924631e-06,\n",
       "         3.30226555e-07, -3.39291614e-06,  1.49045138e-06,  3.53807263e-06,\n",
       "         6.16204125e-06,  3.64372931e-06, -1.99781311e-06,  1.49438438e-06],\n",
       "       dtype=float32),\n",
       " array([[ 8.05704549e-05, -2.04092830e-05,  3.58427642e-05, ...,\n",
       "         -1.88223737e-07, -1.83567522e-07, -1.84370123e-07],\n",
       "        [ 7.35347130e-05,  5.34300234e-05,  7.15629867e-05, ...,\n",
       "         -1.09836549e-07, -1.07092909e-07, -1.07571623e-07],\n",
       "        [-6.10279785e-05,  9.32887924e-05,  1.83470984e-04, ...,\n",
       "         -5.21042409e-09, -5.07175280e-09, -5.05374942e-09],\n",
       "        ...,\n",
       "        [-2.76823848e-04, -3.21114261e-04, -1.25742110e-04, ...,\n",
       "          3.01320625e-07,  2.93908897e-07,  2.95170707e-07],\n",
       "        [ 1.18199474e-04,  1.59320232e-04,  5.92672332e-05, ...,\n",
       "         -9.11427946e-08, -8.89419169e-08, -8.92789984e-08],\n",
       "        [ 5.48534445e-08, -1.79966941e-04, -9.49650566e-05, ...,\n",
       "          1.81967934e-07,  1.77508596e-07,  1.78249479e-07]], dtype=float32),\n",
       " array([-0.78131634, -1.0979933 , -0.9813314 , ...,  0.00201733,\n",
       "         0.00196766,  0.00197582], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, action_type):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n",
    "        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "        # This is an optimization of the LSTM output, but is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ####################################################################################################\n",
    "        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "            \n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    \n",
    "#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "#                                                       [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if action_type != \"is_training\":\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 4518.902 speed: 27009 wps\n",
      "Itr 87 of 774, perplexity: 1288.968 speed: 42533 wps\n",
      "Itr 164 of 774, perplexity: 993.294 speed: 44599 wps\n",
      "Itr 241 of 774, perplexity: 826.965 speed: 45983 wps\n",
      "Itr 318 of 774, perplexity: 731.302 speed: 46460 wps\n",
      "Itr 395 of 774, perplexity: 654.226 speed: 46577 wps\n",
      "Itr 472 of 774, perplexity: 592.830 speed: 46795 wps\n",
      "Itr 549 of 774, perplexity: 538.000 speed: 46641 wps\n",
      "Itr 626 of 774, perplexity: 494.684 speed: 46679 wps\n",
      "Itr 703 of 774, perplexity: 459.916 speed: 46710 wps\n",
      "Epoch 1 : Train Perplexity: 435.015\n",
      "Epoch 1 : Valid Perplexity: 267.982\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 280.873 speed: 34253 wps\n",
      "Itr 87 of 774, perplexity: 240.522 speed: 46761 wps\n",
      "Itr 164 of 774, perplexity: 230.361 speed: 47176 wps\n",
      "Itr 241 of 774, perplexity: 220.485 speed: 47814 wps\n",
      "Itr 318 of 774, perplexity: 217.459 speed: 47836 wps\n",
      "Itr 395 of 774, perplexity: 211.621 speed: 47924 wps\n",
      "Itr 472 of 774, perplexity: 207.143 speed: 47854 wps\n",
      "Itr 549 of 774, perplexity: 200.328 speed: 47681 wps\n",
      "Itr 626 of 774, perplexity: 194.725 speed: 47728 wps\n",
      "Itr 703 of 774, perplexity: 190.463 speed: 47694 wps\n",
      "Epoch 2 : Train Perplexity: 187.784\n",
      "Epoch 2 : Valid Perplexity: 177.407\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 187.056 speed: 32413 wps\n",
      "Itr 87 of 774, perplexity: 160.656 speed: 44154 wps\n",
      "Itr 164 of 774, perplexity: 156.961 speed: 45404 wps\n",
      "Itr 241 of 774, perplexity: 152.467 speed: 45829 wps\n",
      "Itr 318 of 774, perplexity: 152.493 speed: 46303 wps\n",
      "Itr 395 of 774, perplexity: 149.845 speed: 46669 wps\n",
      "Itr 472 of 774, perplexity: 148.178 speed: 46887 wps\n",
      "Itr 549 of 774, perplexity: 144.457 speed: 46845 wps\n",
      "Itr 626 of 774, perplexity: 141.554 speed: 46812 wps\n",
      "Itr 703 of 774, perplexity: 139.671 speed: 46824 wps\n",
      "Epoch 3 : Train Perplexity: 138.649\n",
      "Epoch 3 : Valid Perplexity: 157.902\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 151.153 speed: 33834 wps\n",
      "Itr 87 of 774, perplexity: 128.078 speed: 44824 wps\n",
      "Itr 164 of 774, perplexity: 125.956 speed: 46023 wps\n",
      "Itr 241 of 774, perplexity: 122.666 speed: 46325 wps\n",
      "Itr 318 of 774, perplexity: 123.382 speed: 46705 wps\n",
      "Itr 395 of 774, perplexity: 121.546 speed: 46590 wps\n",
      "Itr 472 of 774, perplexity: 120.673 speed: 46445 wps\n",
      "Itr 549 of 774, perplexity: 117.884 speed: 46538 wps\n",
      "Itr 626 of 774, perplexity: 115.930 speed: 46717 wps\n",
      "Itr 703 of 774, perplexity: 114.809 speed: 46542 wps\n",
      "Epoch 4 : Train Perplexity: 114.312\n",
      "Epoch 4 : Valid Perplexity: 143.661\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 128.383 speed: 37479 wps\n",
      "Itr 87 of 774, perplexity: 109.034 speed: 46455 wps\n",
      "Itr 164 of 774, perplexity: 107.735 speed: 46759 wps\n",
      "Itr 241 of 774, perplexity: 105.040 speed: 46390 wps\n",
      "Itr 318 of 774, perplexity: 105.858 speed: 46230 wps\n",
      "Itr 395 of 774, perplexity: 104.454 speed: 46394 wps\n",
      "Itr 472 of 774, perplexity: 103.931 speed: 46571 wps\n",
      "Itr 549 of 774, perplexity: 101.592 speed: 46564 wps\n",
      "Itr 626 of 774, perplexity: 100.092 speed: 46630 wps\n",
      "Itr 703 of 774, perplexity: 99.266 speed: 46574 wps\n",
      "Epoch 5 : Train Perplexity: 98.965\n",
      "Epoch 5 : Valid Perplexity: 140.550\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "Itr 10 of 774, perplexity: 109.724 speed: 36728 wps\n",
      "Itr 87 of 774, perplexity: 92.790 speed: 45754 wps\n",
      "Itr 164 of 774, perplexity: 90.982 speed: 46757 wps\n",
      "Itr 241 of 774, perplexity: 88.102 speed: 46986 wps\n",
      "Itr 318 of 774, perplexity: 88.335 speed: 46748 wps\n",
      "Itr 395 of 774, perplexity: 86.591 speed: 46343 wps\n",
      "Itr 472 of 774, perplexity: 85.793 speed: 46294 wps\n",
      "Itr 549 of 774, perplexity: 83.399 speed: 46139 wps\n",
      "Itr 626 of 774, perplexity: 81.750 speed: 46059 wps\n",
      "Itr 703 of 774, perplexity: 80.678 speed: 46128 wps\n",
      "Epoch 6 : Train Perplexity: 80.104\n",
      "Epoch 6 : Valid Perplexity: 127.019\n",
      "Epoch 7 : Learning rate: 0.250\n",
      "Itr 10 of 774, perplexity: 95.190 speed: 30596 wps\n",
      "Itr 87 of 774, perplexity: 81.844 speed: 44282 wps\n",
      "Itr 164 of 774, perplexity: 80.398 speed: 45385 wps\n",
      "Itr 241 of 774, perplexity: 77.767 speed: 45720 wps\n",
      "Itr 318 of 774, perplexity: 77.986 speed: 45883 wps\n",
      "Itr 395 of 774, perplexity: 76.320 speed: 46171 wps\n",
      "Itr 472 of 774, perplexity: 75.525 speed: 46261 wps\n",
      "Itr 549 of 774, perplexity: 73.319 speed: 46420 wps\n",
      "Itr 626 of 774, perplexity: 71.741 speed: 46355 wps\n",
      "Itr 703 of 774, perplexity: 70.670 speed: 46340 wps\n",
      "Epoch 7 : Train Perplexity: 70.032\n",
      "Epoch 7 : Valid Perplexity: 124.307\n",
      "Epoch 8 : Learning rate: 0.125\n",
      "Itr 10 of 774, perplexity: 87.391 speed: 34526 wps\n",
      "Itr 87 of 774, perplexity: 75.729 speed: 45273 wps\n",
      "Itr 164 of 774, perplexity: 74.477 speed: 45938 wps\n",
      "Itr 241 of 774, perplexity: 72.023 speed: 46366 wps\n",
      "Itr 318 of 774, perplexity: 72.289 speed: 46698 wps\n",
      "Itr 395 of 774, perplexity: 70.701 speed: 46863 wps\n",
      "Itr 472 of 774, perplexity: 69.927 speed: 47028 wps\n",
      "Itr 549 of 774, perplexity: 67.828 speed: 47036 wps\n",
      "Itr 626 of 774, perplexity: 66.303 speed: 47146 wps\n",
      "Itr 703 of 774, perplexity: 65.256 speed: 47137 wps\n",
      "Epoch 8 : Train Perplexity: 64.616\n",
      "Epoch 8 : Valid Perplexity: 123.341\n",
      "Epoch 9 : Learning rate: 0.062\n",
      "Itr 10 of 774, perplexity: 83.064 speed: 34127 wps\n",
      "Itr 87 of 774, perplexity: 72.287 speed: 44360 wps\n",
      "Itr 164 of 774, perplexity: 71.110 speed: 45313 wps\n",
      "Itr 241 of 774, perplexity: 68.781 speed: 46190 wps\n",
      "Itr 318 of 774, perplexity: 69.080 speed: 46479 wps\n",
      "Itr 395 of 774, perplexity: 67.560 speed: 46151 wps\n",
      "Itr 472 of 774, perplexity: 66.811 speed: 46098 wps\n",
      "Itr 549 of 774, perplexity: 64.783 speed: 45883 wps\n",
      "Itr 626 of 774, perplexity: 63.300 speed: 45871 wps\n",
      "Itr 703 of 774, perplexity: 62.278 speed: 45909 wps\n",
      "Epoch 9 : Train Perplexity: 61.656\n",
      "Epoch 9 : Valid Perplexity: 122.693\n",
      "Epoch 10 : Learning rate: 0.031\n",
      "Itr 10 of 774, perplexity: 80.805 speed: 36201 wps\n",
      "Itr 87 of 774, perplexity: 70.407 speed: 45235 wps\n",
      "Itr 164 of 774, perplexity: 69.248 speed: 46483 wps\n",
      "Itr 241 of 774, perplexity: 67.002 speed: 46763 wps\n",
      "Itr 318 of 774, perplexity: 67.313 speed: 46562 wps\n",
      "Itr 395 of 774, perplexity: 65.836 speed: 46328 wps\n",
      "Itr 472 of 774, perplexity: 65.101 speed: 46479 wps\n",
      "Itr 549 of 774, perplexity: 63.116 speed: 46575 wps\n",
      "Itr 626 of 774, perplexity: 61.659 speed: 46638 wps\n",
      "Itr 703 of 774, perplexity: 60.653 speed: 46668 wps\n",
      "Epoch 10 : Train Perplexity: 60.042\n",
      "Epoch 10 : Valid Perplexity: 122.286\n",
      "Epoch 11 : Learning rate: 0.016\n",
      "Itr 10 of 774, perplexity: 79.605 speed: 33257 wps\n",
      "Itr 87 of 774, perplexity: 69.396 speed: 44522 wps\n",
      "Itr 164 of 774, perplexity: 68.253 speed: 44847 wps\n",
      "Itr 241 of 774, perplexity: 66.048 speed: 44942 wps\n",
      "Itr 318 of 774, perplexity: 66.357 speed: 45232 wps\n",
      "Itr 395 of 774, perplexity: 64.902 speed: 45650 wps\n",
      "Itr 472 of 774, perplexity: 64.173 speed: 45843 wps\n",
      "Itr 549 of 774, perplexity: 62.210 speed: 45945 wps\n",
      "Itr 626 of 774, perplexity: 60.768 speed: 46010 wps\n",
      "Itr 703 of 774, perplexity: 59.772 speed: 46144 wps\n",
      "Epoch 11 : Train Perplexity: 59.165\n",
      "Epoch 11 : Valid Perplexity: 122.087\n",
      "Epoch 12 : Learning rate: 0.008\n",
      "Itr 10 of 774, perplexity: 78.947 speed: 35392 wps\n",
      "Itr 87 of 774, perplexity: 68.827 speed: 44613 wps\n",
      "Itr 164 of 774, perplexity: 67.716 speed: 45289 wps\n",
      "Itr 241 of 774, perplexity: 65.530 speed: 45496 wps\n",
      "Itr 318 of 774, perplexity: 65.836 speed: 45568 wps\n",
      "Itr 395 of 774, perplexity: 64.393 speed: 45826 wps\n",
      "Itr 472 of 774, perplexity: 63.667 speed: 46070 wps\n",
      "Itr 549 of 774, perplexity: 61.718 speed: 46399 wps\n",
      "Itr 626 of 774, perplexity: 60.284 speed: 46673 wps\n",
      "Itr 703 of 774, perplexity: 59.293 speed: 46855 wps\n",
      "Epoch 12 : Train Perplexity: 58.687\n",
      "Epoch 12 : Valid Perplexity: 121.954\n",
      "Epoch 13 : Learning rate: 0.004\n",
      "Itr 10 of 774, perplexity: 78.574 speed: 33467 wps\n",
      "Itr 87 of 774, perplexity: 68.497 speed: 44877 wps\n",
      "Itr 164 of 774, perplexity: 67.417 speed: 45354 wps\n",
      "Itr 241 of 774, perplexity: 65.245 speed: 45720 wps\n",
      "Itr 318 of 774, perplexity: 65.548 speed: 46298 wps\n",
      "Itr 395 of 774, perplexity: 64.112 speed: 46612 wps\n",
      "Itr 472 of 774, perplexity: 63.392 speed: 46892 wps\n",
      "Itr 549 of 774, perplexity: 61.451 speed: 47094 wps\n",
      "Itr 626 of 774, perplexity: 60.021 speed: 47276 wps\n",
      "Itr 703 of 774, perplexity: 59.033 speed: 47394 wps\n",
      "Epoch 13 : Train Perplexity: 58.428\n",
      "Epoch 13 : Valid Perplexity: 121.820\n",
      "Epoch 14 : Learning rate: 0.002\n",
      "Itr 10 of 774, perplexity: 78.364 speed: 33994 wps\n",
      "Itr 87 of 774, perplexity: 68.309 speed: 45958 wps\n",
      "Itr 164 of 774, perplexity: 67.247 speed: 46222 wps\n",
      "Itr 241 of 774, perplexity: 65.089 speed: 46667 wps\n",
      "Itr 318 of 774, perplexity: 65.392 speed: 46807 wps\n",
      "Itr 395 of 774, perplexity: 63.960 speed: 47082 wps\n",
      "Itr 472 of 774, perplexity: 63.243 speed: 47094 wps\n",
      "Itr 549 of 774, perplexity: 61.309 speed: 47214 wps\n",
      "Itr 626 of 774, perplexity: 59.879 speed: 47359 wps\n",
      "Itr 703 of 774, perplexity: 58.893 speed: 47424 wps\n",
      "Epoch 14 : Train Perplexity: 58.289\n",
      "Epoch 14 : Valid Perplexity: 121.708\n",
      "Epoch 15 : Learning rate: 0.001\n",
      "Itr 10 of 774, perplexity: 78.251 speed: 33613 wps\n",
      "Itr 87 of 774, perplexity: 68.206 speed: 44448 wps\n",
      "Itr 164 of 774, perplexity: 67.153 speed: 45139 wps\n",
      "Itr 241 of 774, perplexity: 65.003 speed: 45299 wps\n",
      "Itr 318 of 774, perplexity: 65.309 speed: 45614 wps\n",
      "Itr 395 of 774, perplexity: 63.879 speed: 46120 wps\n",
      "Itr 472 of 774, perplexity: 63.163 speed: 46204 wps\n",
      "Itr 549 of 774, perplexity: 61.234 speed: 46430 wps\n",
      "Itr 626 of 774, perplexity: 59.804 speed: 46538 wps\n",
      "Itr 703 of 774, perplexity: 58.820 speed: 46581 wps\n",
      "Epoch 15 : Train Perplexity: 58.217\n",
      "Epoch 15 : Valid Perplexity: 121.638\n",
      "Test Perplexity: 116.938\n",
      "Training time in minutes:\n",
      "317.73429918289185\n"
     ]
    }
   ],
   "source": [
    "# Initializes the Execution Graph and the Session\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "    print('Training time in minutes:')\n",
    "    print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
